{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff46f92d-9184-4ca9-91fe-7b1c7d84366e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3003 entries, 0 to 2\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   fecha         3003 non-null   datetime64[ns]\n",
      " 1   CodCliente    3003 non-null   float64       \n",
      " 2   Importe       3003 non-null   float64       \n",
      " 3   Comisión      2994 non-null   float64       \n",
      " 4   Producto      3003 non-null   object        \n",
      " 5   Region        3003 non-null   object        \n",
      " 6   AumentaSaldo  3003 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(3), object(3)\n",
      "memory usage: 187.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#Importamos librerias\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#import os\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import scipy.stats as stats\n",
    "#from datetime import datetime\n",
    "#from sklearn.preprocessing import StandardScaler, OneHotEncoder,MinMaxScaler\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "#from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,roc_curve, auc,precision_recall_curve, average_precision_score\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuración básica de log\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='app_AIMLK_M02T03.log',  # Guardar logs en un archivo\n",
    "    filemode='w'  # Sobreescribe el archivo en cada ejecución\n",
    ")\n",
    "\n",
    "def ProcesoAutomatizado():\n",
    "    logging.info(\"#Generando semilla\\n\")\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    logging.info(\"#Creando datos de ejemplo\\n\")\n",
    "    datos = {\n",
    "        'fecha': pd.date_range(start='2023-12-08', periods=3000, freq='D'),\n",
    "        'CodCliente': np.random.normal(100000, 500, 3000),\n",
    "        'Importe': np.random.normal(49400, 500, 3000),\n",
    "        'Comisión': np.random.normal(494, 150, 3000),\n",
    "        'Producto': np.random.choice(['Compra BitCoin', 'Cuenta Ahorro', 'Cuenta Corriente', 'DPF'], 3000),\n",
    "        'Region': np.random.choice(['San Salvador Norte', 'San Salvador Sur', 'San Salvador Este', 'San Salvador Oeste'], 3000),\n",
    "        'AumentaSaldo': np.random.choice(['Si', 'No'], 3000)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(datos)\n",
    "    \n",
    "    # Simulamos algunos valores nulos y duplicados para el ejemplo\n",
    "    df.loc[0:5, 'Comisión'] = np.nan\n",
    "    df = pd.concat([df, df.iloc[0:3]])\n",
    "    \n",
    "    logging.info(\"#Mostrando datos originales:\\n\")\n",
    "    logging.info(df)\n",
    "    \n",
    "    logging.info('\\nInformación de Columnas:\\n')\n",
    "    logging.info(df.info())\n",
    "\n",
    "    logging.info('# Creamos una copia para no modificar los datos originales')\n",
    "    df_limpio = df.copy()\n",
    "    \n",
    "    logging.info('# 1. Manejo de valores faltantes')\n",
    "    logging.info('# Rellenamos valores faltantes en columnas numéricas con la mediana')\n",
    "    columnas_numericas = df_limpio.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for columna in columnas_numericas:\n",
    "        if df_limpio[columna].isnull().sum() > 0:\n",
    "            mediana = df_limpio[columna].median()\n",
    "            #df_limpio[columna].fillna(mediana, inplace=True)\n",
    "            df_limpio[columna] = df_limpio[columna].fillna(mediana)\n",
    "            logging.debug(f\"- Valores faltantes en '{columna}' rellenados con la mediana: {mediana}\")\n",
    "\n",
    "    logging.info('# 2. Eliminación de duplicados')\n",
    "    duplicados = df_limpio.duplicated().sum()\n",
    "    df_limpio.drop_duplicates(inplace=True)\n",
    "    logging.debug(f\"- Se eliminaron {duplicados} registros duplicados\")\n",
    "\n",
    "    logging.info('# 3. Estandarización de texto')\n",
    "    columnas_texto = df_limpio.select_dtypes(include=['object']).columns\n",
    "    for columna in columnas_texto:\n",
    "        # Convertimos a minúsculas y eliminamos espacios externos\n",
    "        df_limpio[columna] = df_limpio[columna].str.lower().str.strip()\n",
    "        logging.debug(f\"- Columna '{columna}' estandarizada a minúsculas y sin espacios externos\")\n",
    "\n",
    "    logging.info('# 4. Manejo de valores atípicos (outliers)')\n",
    "    for columna in columnas_numericas:\n",
    "        Q1 = df_limpio[columna].quantile(0.25)\n",
    "        Q3 = df_limpio[columna].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        limite_inferior = Q1 - 1.5 * IQR\n",
    "        limite_superior = Q3 + 1.5 * IQR\n",
    "    \n",
    "        # Identificamos outliers\n",
    "        outliers = df_limpio[(df_limpio[columna] < limite_inferior) | \n",
    "                            (df_limpio[columna] > limite_superior)][columna]\n",
    "        \n",
    "        if len(outliers) > 0:\n",
    "            # Recortamos los valores atípicos a los límites\n",
    "            df_limpio[columna] = df_limpio[columna].clip(limite_inferior, limite_superior)\n",
    "            logging.debug(f\"- Se encontraron y trataron {len(outliers)} valores atípicos en '{columna}'\")\n",
    "            \n",
    "    logging.info('# 5. Creación de nuevas características')\n",
    "    # Ejemplo: Calcular ratios financieros\n",
    "    if 'Importe' in df_limpio.columns and 'Comisión' in df_limpio.columns:\n",
    "        df_limpio['margen_beneficio'] = ((df_limpio['Importe'] - df_limpio['Comisión']) / \n",
    "                                        df_limpio['Importe'] * 100)\n",
    "        logging.debug(\"- Creada nueva columna 'margen_beneficio'\")\n",
    "\n",
    "    logging.info('# 6. Normalización de datos numéricos')\n",
    "    for columna in columnas_numericas:\n",
    "        # Aplicamos normalización Min-Max\n",
    "        min_val = df_limpio[columna].min()\n",
    "        max_val = df_limpio[columna].max()\n",
    "        df_limpio[f'{columna}_normalizado'] = ((df_limpio[columna] - min_val) / \n",
    "                                              (max_val - min_val))\n",
    "        logging.debug(f\"- Columna '{columna}' normalizada entre 0 y 1\")\n",
    "\n",
    "    logging.info('# 7. Conversión de tipos de datos')\n",
    "    logging.info('# Preparando el formato de fecha')\n",
    "    date_format = '%Y-%m-%d'\n",
    "    \n",
    "    logging.info('# Convertir columnas de fecha')\n",
    "    for columna in columnas_texto:\n",
    "        try:\n",
    "            df_limpio[f'{columna}_fecha'] = pd.to_datetime(df_limpio[columna], format=date_format)\n",
    "            logging.info(f\"- Columna '{columna}' convertida a formato fecha\")\n",
    "        except:\n",
    "            logging.debug(f\"- La Columna '{columna}' no es de tipo fecha\")\n",
    "            continue\n",
    "\n",
    "\n",
    "    logging.info(\"# 8. Seleccionar columnas necesarias\\n\")\n",
    "    final_dataset = df_limpio[['Importe', 'Comisión', 'Producto', 'Region', 'AumentaSaldo']]\n",
    "    \n",
    "    \n",
    "    logging.info(\"# 9. Separar las características (X) y la variable objetivo (y)\")\n",
    "    logging.info(\"Distribución de la variable objetivo (AumentaSaldo):\")\n",
    "    logging.info(final_dataset['AumentaSaldo'].value_counts())\n",
    "    logging.debug(final_dataset['AumentaSaldo'].unique())\n",
    "    X = final_dataset.drop(columns=['AumentaSaldo'])\n",
    "    y = final_dataset['AumentaSaldo'].apply(lambda x: 1 if x == 'si' else 0) # Convertir 'si' a 1 y 'no' a 0 para SMOTE\n",
    "    logging.info(y.value_counts())\n",
    "    \n",
    "    logging.info(\"# 10. Aplicar One-Hot Encoding a las columnas categóricas\\n\")\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first')  # Cambiamos sparse a sparse_output\n",
    "    X_encoded = encoder.fit_transform(X[['Producto', 'Region']])\n",
    "    X_encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(['Producto', 'Region']))\n",
    "\n",
    "    logging.info(\"# 11. Combinar columnas codificadas con las numéricas\\n\")\n",
    "    X_numeric = X[['Importe', 'Comisión']].reset_index(drop=True)\n",
    "    X_final = pd.concat([X_numeric, X_encoded_df], axis=1)\n",
    "\n",
    "    logging.info(\"# 12. Dividir datos en entrenamiento y prueba\\n\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    logging.info(\"# 13. Aplicar SMOTE para balancear clases\\n\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    logging.info(\"# 14. Distribución después de SMOTE:\\n\")\n",
    "    logging.info(y_train_balanced.value_counts())\n",
    "    \n",
    "    logging.info('# 15. XGBoost import XGBClassifier:\\n')\n",
    "    logging.debug('n_estimators=500: Número de árboles en el modelo.')\n",
    "    logging.debug('random_state=42: Fija la semilla para reproducibilidad.')\n",
    "    logging.debug('max_depth=3:  Profundidad máxima de cada árbol.')\n",
    "    logging.debug('eval_metric=''logloss'': Métrica de pérdida logarítmica usada internamente durante el entrenamiento.')\n",
    "    xgb_model = XGBClassifier(n_estimators=500, random_state=42, max_depth=3, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    logging.info('# 16. Predicciones en el conjunto de prueba\\n')\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    logging.info(f\"# 17. Evaluación del modelo\\n\")\n",
    "    logging.info(f\"Accuracy del modelo: {accuracy_score(y_test, y_pred)}\")\n",
    "    logging.info(f\"\\nClassification Report:\\n {classification_report(y_test, y_pred)}\")\n",
    "\n",
    "    logging.info(f\"# 18. Aplicando Epoch\")\n",
    "    logging.info(f\"#     Definir conjuntos de entrenamiento y validación\")\n",
    "    eval_set = [(X_train_balanced, y_train_balanced), (X_test, y_test)]\n",
    "\n",
    "    logging.info(f\"# 19. Entrenar el modelo XGBoost Mejores Parametros\")\n",
    "    #xgb_model = XGBClassifier(n_estimators=500, random_state=42, eval_metric='logloss')\n",
    "    xgb_model = XGBClassifier(n_estimators=300, learning_rate=0.01, max_depth=12,\n",
    "                              colsample_bytree=0.8, subsample=0.6,\n",
    "                             random_state=42, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train_balanced, y_train_balanced, eval_set=eval_set, verbose=False)\n",
    "\n",
    "    logging.info(f\"# 20. Predicciones\")\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    logging.info(f\"\\nClassification Report 2:\\n {classification_report(y_test, y_pred)}\")\n",
    "    \n",
    "    logging.info(f\"# 21. CONCLUSIONES\")\n",
    "    logging.info(f\"Luego de ajustar los parametros del XGBClassifier: \")\n",
    "    logging.info(f\"n_estimators=300 \")\n",
    "    logging.info(f\"learning_rate=0.01 \")\n",
    "    logging.info(f\"max_depth=12 \")\n",
    "    logging.info(f\"colsample_bytree=0.8 \")\n",
    "    logging.info(f\"subsample=0.6 \")\n",
    "    logging.info(f\"random_state=42 \")\n",
    "    logging.info(f\"eval_metric='logloss' \")\n",
    "    logging.info(f\"/n Fue posible mejorar los valores para accuracy, precision y recall!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "def main():\n",
    "\n",
    "    #Titulos \n",
    "    logging.info(\"#-----------------BOF.XGBoots-----------------#\")\n",
    "    logging.info(\"#######################################################\")\n",
    "    logging.info(\"# AIML- INCAF-1 Módulo 3: Diseñar e implementar modelos\")\n",
    "    logging.info(\"## Actividad 1: Construcción de un modelo de Machine Learning utilizando un algoritmo avanzado.\")\n",
    "    logging.info(\"### Elaborado por: Gabriel Guzmán\\n\")\n",
    "    logging.info(\"Se creado un programa Python el cuál se ejecuta de forma automática las tareas de preparación de datos.\")\n",
    "    logging.info(\"Se añade el uso de XGBoost para analizar la posibilidad de nuestra variable target si aumentará saldo o no\")\n",
    "    logging.info(\"\\n\")\n",
    "    logging.info(\"Iniciando Proceso Automatizado...\")\n",
    "    ProcesoAutomatizado()\n",
    "    logging.info(\"Fin del Proceso Automatizado.\")\n",
    "    logging.info(\"#-----------------EOF.XGBoots-----------------#\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e08f37-d388-415f-b830-f7250ee77844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0aecd8-e40b-4cbc-affa-c5d38844044a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
